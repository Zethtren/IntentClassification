{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import urllib\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word vectorizer\n",
    "vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "def get_response(url):\n",
    "    operUrl = urllib.request.urlopen(url)\n",
    "    if(operUrl.getcode()==200):\n",
    "        data = operUrl.read()\n",
    "    else:\n",
    "        print(\"Error receiving data\", operUrl.getcode())\n",
    "    return data\n",
    "\n",
    "data = get_response('https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine data into train, val, and test sets\n",
    "json_data = json.loads(data)\n",
    "val = json_data['val']\n",
    "oos_val = json_data['oos_val']\n",
    "test = json_data['test']\n",
    "oos_test = json_data['oos_test']\n",
    "train = json_data['train']\n",
    "oos_train = json_data['oos_train']\n",
    "\n",
    "\n",
    "for i in oos_train:\n",
    "    train.append(i) \n",
    "\n",
    "for i in oos_val:\n",
    "    val.append(i) \n",
    "\n",
    "for i in oos_test:\n",
    "    test.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Strip data of symbols and convert to lists of strings    \n",
    "# Create regex pattern\n",
    "pattern = re.compile('[^\\w\\s]')\n",
    "\n",
    "# Define function to apply regex pattern over each dataset\n",
    "def string_separation(data):\n",
    "    return [pattern.sub('', data[i][0]).split(\" \") \\\n",
    "     for i, j in enumerate(data)]\n",
    "\n",
    "# Apply function to each dataset\n",
    "train_separated_values = string_separation(train)\n",
    "val_separated_values = string_separation(val)\n",
    "test_separated_values = string_separation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vectorize each value using word2vec\n",
    "\n",
    "# Create function that checks if a string can be vectorized against the vectorizer\n",
    "# If it can, vectorize it, otherwise return a 0.0 array of equal size\n",
    "def get_vector(string):\n",
    "    try: \n",
    "        value = vectors.get_vector(string)\n",
    "        return value\n",
    "    except:\n",
    "        return np.array([0.0] * 50)\n",
    "    \n",
    "# Create a function that applies the get_vector function across a list of values\n",
    "def convert_to_vectors(series):\n",
    "    new_series = []\n",
    "\n",
    "    for i in series:\n",
    "        if i == '':\n",
    "            pass\n",
    "        else:\n",
    "            new_series.append(get_vector(i))\n",
    "            \n",
    "    return new_series\n",
    "\n",
    "# Define a function that applies convert_to_vectors across an entire dataset\n",
    "def vectorize_samples(data):\n",
    "    return [convert_to_vectors(data[i]) for i, j in enumerate(data)]\n",
    "\n",
    "# Vectorize each dataset\n",
    "train_X_arrays = vectorize_samples(train_separated_values)\n",
    "val_X_arrays = vectorize_samples(val_separated_values)\n",
    "test_X_arrays = vectorize_samples(test_separated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since values are being assigned to Tensors establish the storage location by checking for GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish padding length (Could have been as low as 30 given data set 50 was chosen to increase usability)\n",
    "pad_length = 28\n",
    "\n",
    "# Create an empty array to be used for padding values\n",
    "pad_value = np.array([0.0] * 50, dtype=np.float32)\n",
    "\n",
    "# Define function that takes a list, pad_length, and pad_value to create a new array of values with 0 padded values up to the pad_length\n",
    "def adjust_padding(some_list, pad_length=pad_length, pad_value=pad_value):\n",
    "    some_list = list(some_list)\n",
    "    while len(some_list) < pad_length:\n",
    "        some_list.append(pad_value)\n",
    "    return np.array(some_list)\n",
    "\n",
    "# Apply the function over each dataset\n",
    "train_padded      = [adjust_padding(i) for i in train_X_arrays]\n",
    "val_padded        = [adjust_padding(val_X_arrays[i]) for i, j in enumerate(val_X_arrays)]\n",
    "test_padded       = [adjust_padding(test_X_arrays[i]) for i, j in enumerate(test_X_arrays)]\n",
    "\n",
    "# Convert arrays to tensors to feed model\n",
    "\n",
    "train_set         = torch.tensor(train_padded).reshape((15100,28,50)).float().to(device)\n",
    "val_set           = torch.tensor(val_padded).reshape((3100,28,50)).float().to(device)\n",
    "# test_set          = torch.tensor(test_padded).reshape((5500,28,50)).float().to(device)\n",
    "\n",
    "# Create tensors of answers\n",
    "\n",
    "train_answer      = [j[1] for i, j in enumerate(train)]\n",
    "val_answer        = [j[1] for i, j in enumerate(val)]\n",
    "test_answer       = [j[1] for i, j in enumerate(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create an index to train against\n",
    "\n",
    "# Create dictionary map of each unique answer type\n",
    "sorted_list = list(set([train[i][1] for i, j in enumerate(train)]))\n",
    "sorted_list.sort()\n",
    "chosen_dict = {j: i for i, j in enumerate(sorted_list)}\n",
    "\n",
    "# Use dictionary to consistently label answer data\n",
    "for item in chosen_dict.keys():\n",
    "    for i, j in enumerate(train_answer):\n",
    "        if item == j:\n",
    "            train_answer[i] = chosen_dict[item]\n",
    "    for i, j in enumerate(val_answer):\n",
    "        if item == j:\n",
    "            val_answer[i] = chosen_dict[item]\n",
    "    for i, j in enumerate(test_answer):\n",
    "        if item == j:\n",
    "            test_answer[i] = chosen_dict[item]\n",
    "\n",
    "# Convert labeled data into tensors\n",
    "train_answers = torch.tensor(train_answer).to(device)\n",
    "val_answers = torch.tensor(val_answer).to(device)\n",
    "# test_answers = torch.tensor(test_answer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ClassifierLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(batch_first=True, input_size=50, hidden_size=256, bidirectional=True)\n",
    "\n",
    "\n",
    "        self.linear1 = nn.Linear(256, 256)\n",
    "        self.linear1_2 = nn.Linear(256, 256)\n",
    "        self.probs_pre = nn.Linear(512, 151) # 151 output classes\n",
    "        self.probabilities = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        hidden1 = hidden[0]\n",
    "        hidden2 = hidden[1]\n",
    "        # Take hidden layer as output. \n",
    "        x = F.elu(self.dropout(self.linear1(hidden1)))\n",
    "        x_2 = F.elu(self.dropout(self.linear1_2(hidden2)))\n",
    "        x = torch.cat((x, x_2), 1)\n",
    "        x = self.probabilities(self.probs_pre(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierLSTM().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, amsgrad=True)\n",
    "\n",
    "\n",
    "avg_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:     5.017273333333335\n",
      "Val Loss: 5.0172\n",
      "0.0% done\n",
      "Loss:     4.984243333333334\n",
      "Val Loss: 4.9861\n",
      "0.2% done\n",
      "Loss:     4.9507433333333335\n",
      "Val Loss: 4.9588\n",
      "0.4% done\n",
      "Loss:     4.8729233333333335\n",
      "Val Loss: 4.8951\n",
      "0.6% done\n",
      "Loss:     4.80422\n",
      "Val Loss: 4.8433\n",
      "0.8% done\n",
      "Loss:     4.7937233333333324\n",
      "Val Loss: 4.8357\n",
      "1.0% done\n",
      "Loss:     4.790386666666666\n",
      "Val Loss: 4.8338\n",
      "1.2% done\n",
      "Loss:     4.786730000000001\n",
      "Val Loss: 4.8317\n",
      "1.4% done\n",
      "Loss:     4.77861\n",
      "Val Loss: 4.8230\n",
      "1.6% done\n",
      "Loss:     4.769313333333333\n",
      "Val Loss: 4.8126\n",
      "1.8% done\n",
      "Loss:     4.755893333333334\n",
      "Val Loss: 4.8021\n",
      "2.0% done\n",
      "Loss:     4.754903333333334\n",
      "Val Loss: 4.7993\n",
      "2.2% done\n",
      "Loss:     4.754256666666667\n",
      "Val Loss: 4.7981\n",
      "2.4% done\n",
      "Loss:     4.735033333333335\n",
      "Val Loss: 4.7810\n",
      "2.6% done\n",
      "Loss:     4.704486666666667\n",
      "Val Loss: 4.7530\n",
      "2.8% done\n",
      "Loss:     4.690203333333334\n",
      "Val Loss: 4.7437\n",
      "3.0% done\n",
      "Loss:     4.687209999999999\n",
      "Val Loss: 4.7396\n",
      "3.2% done\n",
      "Loss:     4.6701533333333325\n",
      "Val Loss: 4.7282\n",
      "3.4% done\n",
      "Loss:     4.653266666666666\n",
      "Val Loss: 4.7128\n",
      "3.6% done\n",
      "Loss:     4.645856666666668\n",
      "Val Loss: 4.7067\n",
      "3.8% done\n",
      "Loss:     4.635693333333333\n",
      "Val Loss: 4.7028\n",
      "4.0% done\n",
      "Loss:     4.612870000000001\n",
      "Val Loss: 4.6755\n",
      "4.2% done\n",
      "Loss:     4.599186666666667\n",
      "Val Loss: 4.6711\n",
      "4.4% done\n",
      "Loss:     4.586496666666667\n",
      "Val Loss: 4.6563\n",
      "4.6% done\n",
      "Loss:     4.580043333333333\n",
      "Val Loss: 4.6536\n",
      "4.8% done\n",
      "Loss:     4.5670166666666665\n",
      "Val Loss: 4.6457\n",
      "5.0% done\n",
      "Loss:     4.553663333333334\n",
      "Val Loss: 4.6317\n",
      "5.2% done\n",
      "Loss:     4.53492\n",
      "Val Loss: 4.6185\n",
      "5.4% done\n",
      "Loss:     4.534273333333333\n",
      "Val Loss: 4.6154\n",
      "5.6% done\n",
      "Loss:     4.5302066666666665\n",
      "Val Loss: 4.6182\n",
      "5.8% done\n",
      "Loss:     4.5276466666666675\n",
      "Val Loss: 4.6122\n",
      "6.0% done\n",
      "Loss:     4.508696666666666\n",
      "Val Loss: 4.5981\n",
      "6.2% done\n",
      "Loss:     4.501776666666667\n",
      "Val Loss: 4.5905\n",
      "6.4% done\n",
      "Loss:     4.49539\n",
      "Val Loss: 4.5847\n",
      "6.6% done\n",
      "Loss:     4.488946666666667\n",
      "Val Loss: 4.5802\n",
      "6.8% done\n",
      "Loss:     4.476323333333332\n",
      "Val Loss: 4.5697\n",
      "7.0% done\n",
      "Loss:     4.47609\n",
      "Val Loss: 4.5673\n",
      "7.2% done\n",
      "Loss:     4.475790000000001\n",
      "Val Loss: 4.5700\n",
      "7.4% done\n",
      "Loss:     4.463303333333333\n",
      "Val Loss: 4.5605\n",
      "7.6% done\n",
      "Loss:     4.459056666666667\n",
      "Val Loss: 4.5554\n",
      "7.8% done\n",
      "Loss:     4.443830000000001\n",
      "Val Loss: 4.5405\n",
      "8.0% done\n",
      "Loss:     4.432613333333333\n",
      "Val Loss: 4.5369\n",
      "8.2% done\n",
      "Loss:     4.4241600000000005\n",
      "Val Loss: 4.5286\n",
      "8.4% done\n",
      "Loss:     4.423843333333334\n",
      "Val Loss: 4.5289\n",
      "8.6% done\n",
      "Loss:     4.423726666666666\n",
      "Val Loss: 4.5282\n",
      "8.8% done\n",
      "Loss:     4.42365\n",
      "Val Loss: 4.5280\n",
      "9.0% done\n",
      "Loss:     4.412419999999999\n",
      "Val Loss: 4.5270\n",
      "9.2% done\n",
      "Loss:     4.4108\n",
      "Val Loss: 4.5173\n",
      "9.4% done\n",
      "Loss:     4.410726666666667\n",
      "Val Loss: 4.5169\n",
      "9.6% done\n",
      "Loss:     4.40458\n",
      "Val Loss: 4.5158\n",
      "9.8% done\n",
      "Loss:     4.4043866666666664\n",
      "Val Loss: 4.5108\n",
      "10.0% done\n",
      "Loss:     4.399353333333334\n",
      "Val Loss: 4.5105\n",
      "10.2% done\n",
      "Loss:     4.385246666666666\n",
      "Val Loss: 4.4985\n",
      "10.4% done\n",
      "Loss:     4.384803333333334\n",
      "Val Loss: 4.4974\n",
      "10.6% done\n",
      "Loss:     4.384633333333334\n",
      "Val Loss: 4.4967\n",
      "10.8% done\n",
      "Loss:     4.384533333333334\n",
      "Val Loss: 4.4959\n",
      "11.0% done\n",
      "Loss:     4.360576666666668\n",
      "Val Loss: 4.4849\n",
      "11.2% done\n",
      "Loss:     4.35954\n",
      "Val Loss: 4.4768\n",
      "11.4% done\n",
      "Loss:     4.352993333333333\n",
      "Val Loss: 4.4738\n",
      "11.6% done\n",
      "Loss:     4.352766666666667\n",
      "Val Loss: 4.4717\n",
      "11.8% done\n",
      "Loss:     4.352646666666667\n",
      "Val Loss: 4.4707\n",
      "12.0% done\n",
      "Loss:     4.34005\n",
      "Val Loss: 4.4653\n",
      "12.2% done\n",
      "Loss:     4.33445\n",
      "Val Loss: 4.4609\n",
      "12.4% done\n",
      "Loss:     4.31581\n",
      "Val Loss: 4.4538\n",
      "12.6% done\n",
      "Loss:     4.313633333333333\n",
      "Val Loss: 4.4456\n",
      "12.8% done\n",
      "Loss:     4.307153333333334\n",
      "Val Loss: 4.4376\n",
      "13.0% done\n",
      "Loss:     4.3011566666666665\n",
      "Val Loss: 4.4363\n",
      "13.2% done\n",
      "Loss:     4.300533333333334\n",
      "Val Loss: 4.4345\n",
      "13.4% done\n",
      "Loss:     4.29434\n",
      "Val Loss: 4.4243\n",
      "13.6% done\n",
      "Loss:     4.294210000000001\n",
      "Val Loss: 4.4278\n",
      "13.8% done\n",
      "Loss:     4.293516666666667\n",
      "Val Loss: 4.4234\n",
      "14.0% done\n",
      "Loss:     4.287193333333334\n",
      "Val Loss: 4.4207\n",
      "14.2% done\n",
      "Loss:     4.281273333333333\n",
      "Val Loss: 4.4169\n",
      "14.4% done\n",
      "Loss:     4.281203333333334\n",
      "Val Loss: 4.4169\n",
      "14.6% done\n",
      "Loss:     4.280803333333334\n",
      "Val Loss: 4.4143\n",
      "14.8% done\n",
      "Loss:     4.2687066666666675\n",
      "Val Loss: 4.4112\n",
      "15.0% done\n",
      "Loss:     4.249243333333333\n",
      "Val Loss: 4.3968\n",
      "15.2% done\n",
      "Loss:     4.2487\n",
      "Val Loss: 4.3913\n",
      "15.4% done\n",
      "Loss:     4.2422933333333335\n",
      "Val Loss: 4.3903\n",
      "15.6% done\n",
      "Loss:     4.24209\n",
      "Val Loss: 4.3848\n",
      "15.8% done\n",
      "Loss:     4.236999999999999\n",
      "Val Loss: 4.3903\n",
      "16.0% done\n",
      "Loss:     4.22926\n",
      "Val Loss: 4.3787\n",
      "16.2% done\n",
      "Loss:     4.222720000000001\n",
      "Val Loss: 4.3773\n",
      "16.4% done\n",
      "Loss:     4.222550000000001\n",
      "Val Loss: 4.3747\n",
      "16.6% done\n",
      "Loss:     4.2223999999999995\n",
      "Val Loss: 4.3760\n",
      "16.8% done\n",
      "Loss:     4.216236666666666\n",
      "Val Loss: 4.3746\n",
      "17.0% done\n",
      "Loss:     4.21595\n",
      "Val Loss: 4.3728\n",
      "17.2% done\n",
      "Loss:     4.215886666666666\n",
      "Val Loss: 4.3724\n",
      "17.4% done\n",
      "Loss:     4.215793333333333\n",
      "Val Loss: 4.3688\n",
      "17.6% done\n",
      "Loss:     4.211023333333333\n",
      "Val Loss: 4.3734\n",
      "17.8% done\n",
      "Loss:     4.20939\n",
      "Val Loss: 4.3671\n",
      "18.0% done\n",
      "Loss:     4.20931\n",
      "Val Loss: 4.3675\n",
      "18.2% done\n",
      "Loss:     4.209206666666667\n",
      "Val Loss: 4.3663\n",
      "18.4% done\n",
      "Loss:     4.209196666666666\n",
      "Val Loss: 4.3653\n",
      "18.6% done\n",
      "Loss:     4.206933333333334\n",
      "Val Loss: 4.3672\n",
      "18.8% done\n",
      "Loss:     4.202839999999999\n",
      "Val Loss: 4.3608\n",
      "19.0% done\n",
      "Loss:     4.202663333333333\n",
      "Val Loss: 4.3607\n",
      "19.2% done\n",
      "Loss:     4.1857066666666665\n",
      "Val Loss: 4.3548\n",
      "19.4% done\n",
      "Loss:     4.18319\n",
      "Val Loss: 4.3430\n",
      "19.6% done\n",
      "Loss:     4.183126666666666\n",
      "Val Loss: 4.3482\n",
      "19.8% done\n",
      "Loss:     4.1831\n",
      "Val Loss: 4.3451\n",
      "20.0% done\n",
      "Loss:     4.183083333333332\n",
      "Val Loss: 4.3452\n",
      "20.2% done\n",
      "Loss:     4.177463333333333\n",
      "Val Loss: 4.3456\n",
      "20.4% done\n",
      "Loss:     4.1709\n",
      "Val Loss: 4.3455\n",
      "20.6% done\n",
      "Loss:     4.163946666666666\n",
      "Val Loss: 4.3308\n",
      "20.8% done\n",
      "Loss:     4.15783\n",
      "Val Loss: 4.3322\n",
      "21.0% done\n",
      "Loss:     4.157543333333333\n",
      "Val Loss: 4.3262\n",
      "21.2% done\n",
      "Loss:     4.157463333333333\n",
      "Val Loss: 4.3260\n",
      "21.4% done\n",
      "Loss:     4.157399999999999\n",
      "Val Loss: 4.3281\n",
      "21.6% done\n",
      "Loss:     4.1573166666666665\n",
      "Val Loss: 4.3266\n",
      "21.8% done\n",
      "Loss:     4.157293333333333\n",
      "Val Loss: 4.3238\n",
      "22.0% done\n",
      "Loss:     4.150966666666666\n",
      "Val Loss: 4.3236\n",
      "22.2% done\n",
      "Loss:     4.144459999999999\n",
      "Val Loss: 4.3163\n",
      "22.4% done\n",
      "Loss:     4.138173333333333\n",
      "Val Loss: 4.3114\n",
      "22.6% done\n",
      "Loss:     4.138049999999999\n",
      "Val Loss: 4.3085\n",
      "22.8% done\n",
      "Loss:     4.1317699999999995\n",
      "Val Loss: 4.3074\n",
      "23.0% done\n",
      "Loss:     4.131479999999999\n",
      "Val Loss: 4.3007\n",
      "23.2% done\n",
      "Loss:     4.131386666666667\n",
      "Val Loss: 4.3012\n",
      "23.4% done\n",
      "Loss:     4.13133\n",
      "Val Loss: 4.3013\n",
      "23.6% done\n",
      "Loss:     4.124886666666666\n",
      "Val Loss: 4.2985\n",
      "23.8% done\n",
      "Loss:     4.124846666666666\n",
      "Val Loss: 4.2953\n",
      "24.0% done\n",
      "Loss:     4.1248499999999995\n",
      "Val Loss: 4.2959\n",
      "24.2% done\n",
      "Loss:     4.124826666666666\n",
      "Val Loss: 4.2956\n",
      "24.4% done\n",
      "Loss:     4.124836666666665\n",
      "Val Loss: 4.2950\n",
      "24.6% done\n",
      "Loss:     4.1248233333333335\n",
      "Val Loss: 4.2924\n",
      "24.8% done\n",
      "Loss:     4.12482\n",
      "Val Loss: 4.2955\n",
      "25.0% done\n",
      "Loss:     4.124826666666666\n",
      "Val Loss: 4.2943\n",
      "25.2% done\n",
      "Loss:     4.1188899999999995\n",
      "Val Loss: 4.2971\n",
      "25.4% done\n",
      "Loss:     4.118626666666667\n",
      "Val Loss: 4.2879\n",
      "25.6% done\n",
      "Loss:     4.118606666666666\n",
      "Val Loss: 4.2891\n",
      "25.8% done\n",
      "Loss:     4.1186066666666665\n",
      "Val Loss: 4.2887\n",
      "26.0% done\n",
      "Loss:     4.118473333333332\n",
      "Val Loss: 4.2875\n",
      "26.2% done\n",
      "Loss:     4.118463333333334\n",
      "Val Loss: 4.2866\n",
      "26.4% done\n",
      "Loss:     4.1183966666666665\n",
      "Val Loss: 4.2884\n",
      "26.6% done\n",
      "Loss:     4.112313333333333\n",
      "Val Loss: 4.2906\n",
      "26.8% done\n",
      "Loss:     4.112049999999999\n",
      "Val Loss: 4.2829\n",
      "27.0% done\n",
      "Loss:     4.112033333333333\n",
      "Val Loss: 4.2839\n",
      "27.2% done\n",
      "Loss:     4.105846666666666\n",
      "Val Loss: 4.2825\n",
      "27.4% done\n",
      "Loss:     4.105569999999999\n",
      "Val Loss: 4.2797\n",
      "27.6% done\n",
      "Loss:     4.105553333333333\n",
      "Val Loss: 4.2753\n",
      "27.8% done\n",
      "Loss:     4.105549999999999\n",
      "Val Loss: 4.2759\n",
      "28.0% done\n",
      "Loss:     4.099176666666667\n",
      "Val Loss: 4.2830\n",
      "28.2% done\n",
      "Loss:     4.098946666666666\n",
      "Val Loss: 4.2732\n",
      "28.4% done\n",
      "Loss:     4.0924933333333335\n",
      "Val Loss: 4.2707\n",
      "28.6% done\n",
      "Loss:     4.092459999999999\n",
      "Val Loss: 4.2674\n",
      "28.8% done\n",
      "Loss:     4.092456666666666\n",
      "Val Loss: 4.2666\n",
      "29.0% done\n",
      "Loss:     4.092446666666666\n",
      "Val Loss: 4.2670\n",
      "29.2% done\n",
      "Loss:     4.092443333333333\n",
      "Val Loss: 4.2669\n",
      "29.4% done\n",
      "Loss:     4.092436666666667\n",
      "Val Loss: 4.2691\n",
      "29.6% done\n",
      "Loss:     4.092433333333333\n",
      "Val Loss: 4.2654\n",
      "29.8% done\n",
      "Loss:     4.092426666666666\n",
      "Val Loss: 4.2664\n",
      "30.0% done\n",
      "Loss:     4.092433333333333\n",
      "Val Loss: 4.2683\n",
      "30.2% done\n",
      "Loss:     4.092366666666666\n",
      "Val Loss: 4.2657\n",
      "30.4% done\n",
      "Loss:     4.086036666666667\n",
      "Val Loss: 4.2670\n",
      "30.6% done\n",
      "Loss:     4.085953333333333\n",
      "Val Loss: 4.2638\n",
      "30.8% done\n",
      "Loss:     4.085953333333333\n",
      "Val Loss: 4.2660\n",
      "31.0% done\n",
      "Loss:     4.085943333333333\n",
      "Val Loss: 4.2652\n",
      "31.2% done\n",
      "Loss:     4.0794999999999995\n",
      "Val Loss: 4.2620\n",
      "31.4% done\n",
      "Loss:     4.079456666666666\n",
      "Val Loss: 4.2578\n",
      "31.6% done\n",
      "Loss:     4.079456666666666\n",
      "Val Loss: 4.2576\n",
      "31.8% done\n",
      "Loss:     4.079456666666666\n",
      "Val Loss: 4.2591\n",
      "32.0% done\n",
      "Loss:     4.073006666666666\n",
      "Val Loss: 4.2575\n",
      "32.2% done\n",
      "Loss:     4.072979999999999\n",
      "Val Loss: 4.2554\n",
      "32.4% done\n",
      "Loss:     4.07297\n",
      "Val Loss: 4.2524\n",
      "32.6% done\n",
      "Loss:     4.072973333333334\n",
      "Val Loss: 4.2514\n",
      "32.8% done\n",
      "Loss:     4.0729\n",
      "Val Loss: 4.2514\n",
      "33.0% done\n",
      "Loss:     4.072903333333333\n",
      "Val Loss: 4.2516\n",
      "33.2% done\n",
      "Loss:     4.0729\n",
      "Val Loss: 4.2482\n",
      "33.4% done\n",
      "Loss:     4.072839999999999\n",
      "Val Loss: 4.2503\n",
      "33.6% done\n",
      "Loss:     4.072829999999999\n",
      "Val Loss: 4.2509\n",
      "33.8% done\n",
      "Loss:     4.0728366666666656\n",
      "Val Loss: 4.2516\n",
      "34.0% done\n",
      "Loss:     4.0728366666666656\n",
      "Val Loss: 4.2507\n",
      "34.2% done\n",
      "Loss:     4.072833333333333\n",
      "Val Loss: 4.2487\n",
      "34.4% done\n",
      "Loss:     4.072703333333333\n",
      "Val Loss: 4.2528\n",
      "34.6% done\n",
      "Loss:     4.072699999999999\n",
      "Val Loss: 4.2509\n",
      "34.8% done\n",
      "Loss:     4.066339999999999\n",
      "Val Loss: 4.2507\n",
      "35.0% done\n",
      "Loss:     4.066213333333333\n",
      "Val Loss: 4.2484\n",
      "35.2% done\n",
      "Loss:     4.066209999999999\n",
      "Val Loss: 4.2474\n",
      "35.4% done\n",
      "Loss:     4.0661499999999995\n",
      "Val Loss: 4.2475\n",
      "35.6% done\n",
      "Loss:     4.066146666666667\n",
      "Val Loss: 4.2490\n",
      "35.8% done\n",
      "Loss:     4.0597199999999996\n",
      "Val Loss: 4.2406\n",
      "36.0% done\n",
      "Loss:     4.05967\n",
      "Val Loss: 4.2396\n",
      "36.2% done\n",
      "Loss:     4.059663333333333\n",
      "Val Loss: 4.2431\n",
      "36.4% done\n",
      "Loss:     4.053663333333334\n",
      "Val Loss: 4.2400\n",
      "36.6% done\n",
      "Loss:     4.053319999999999\n",
      "Val Loss: 4.2333\n",
      "36.8% done\n",
      "Loss:     4.048573333333334\n",
      "Val Loss: 4.2372\n",
      "37.0% done\n",
      "Loss:     4.047273333333333\n",
      "Val Loss: 4.2303\n",
      "37.2% done\n",
      "Loss:     4.047209999999999\n",
      "Val Loss: 4.2330\n",
      "37.4% done\n",
      "Loss:     4.040966666666667\n",
      "Val Loss: 4.2359\n",
      "37.6% done\n",
      "Loss:     4.040719999999999\n",
      "Val Loss: 4.2267\n",
      "37.8% done\n",
      "Loss:     4.04066\n",
      "Val Loss: 4.2283\n",
      "38.0% done\n",
      "Loss:     4.040526666666666\n",
      "Val Loss: 4.2302\n",
      "38.2% done\n",
      "Loss:     4.035453333333333\n",
      "Val Loss: 4.2311\n",
      "38.4% done\n",
      "Loss:     4.034886666666666\n",
      "Val Loss: 4.2285\n",
      "38.6% done\n",
      "Loss:     4.034803333333333\n",
      "Val Loss: 4.2262\n",
      "38.8% done\n",
      "Loss:     4.034743333333333\n",
      "Val Loss: 4.2257\n",
      "39.0% done\n",
      "Loss:     4.034739999999999\n",
      "Val Loss: 4.2241\n",
      "39.2% done\n",
      "Loss:     4.034746666666666\n",
      "Val Loss: 4.2235\n",
      "39.4% done\n",
      "Loss:     4.034736666666666\n",
      "Val Loss: 4.2226\n",
      "39.6% done\n",
      "Loss:     4.034679999999999\n",
      "Val Loss: 4.2266\n",
      "39.8% done\n",
      "Loss:     4.034619999999999\n",
      "Val Loss: 4.2248\n",
      "40.0% done\n",
      "Loss:     4.034286666666666\n",
      "Val Loss: 4.2288\n",
      "40.2% done\n",
      "Loss:     4.033836666666667\n",
      "Val Loss: 4.2267\n",
      "40.4% done\n",
      "Loss:     4.033829999999999\n",
      "Val Loss: 4.2265\n",
      "40.6% done\n",
      "Loss:     4.033823333333332\n",
      "Val Loss: 4.2249\n",
      "40.8% done\n",
      "Loss:     4.033763333333332\n",
      "Val Loss: 4.2207\n",
      "41.0% done\n",
      "Loss:     4.033756666666666\n",
      "Val Loss: 4.2220\n",
      "41.2% done\n",
      "Loss:     4.033756666666666\n",
      "Val Loss: 4.2231\n",
      "41.4% done\n",
      "Loss:     4.03377\n",
      "Val Loss: 4.2284\n",
      "41.6% done\n",
      "Loss:     4.033756666666666\n",
      "Val Loss: 4.2281\n",
      "41.8% done\n",
      "Loss:     4.03376\n",
      "Val Loss: 4.2219\n",
      "42.0% done\n",
      "Loss:     4.033759999999999\n",
      "Val Loss: 4.2218\n",
      "42.2% done\n",
      "Loss:     4.033756666666666\n",
      "Val Loss: 4.2240\n",
      "42.4% done\n",
      "Loss:     4.03376\n",
      "Val Loss: 4.2251\n",
      "42.6% done\n",
      "Loss:     4.033743333333333\n",
      "Val Loss: 4.2228\n",
      "42.8% done\n",
      "Loss:     4.0337499999999995\n",
      "Val Loss: 4.2225\n",
      "43.0% done\n",
      "Loss:     4.033753333333333\n",
      "Val Loss: 4.2197\n",
      "43.2% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2220\n",
      "43.4% done\n",
      "Loss:     4.0337499999999995\n",
      "Val Loss: 4.2241\n",
      "43.6% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2233\n",
      "43.8% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2218\n",
      "44.0% done\n",
      "Loss:     4.0337499999999995\n",
      "Val Loss: 4.2200\n",
      "44.2% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2243\n",
      "44.4% done\n",
      "Loss:     4.033753333333333\n",
      "Val Loss: 4.2201\n",
      "44.6% done\n",
      "Loss:     4.0337499999999995\n",
      "Val Loss: 4.2225\n",
      "44.8% done\n",
      "Loss:     4.033753333333333\n",
      "Val Loss: 4.2228\n",
      "45.0% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2231\n",
      "45.2% done\n",
      "Loss:     4.033743333333333\n",
      "Val Loss: 4.2243\n",
      "45.4% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2202\n",
      "45.6% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2244\n",
      "45.8% done\n",
      "Loss:     4.033733333333334\n",
      "Val Loss: 4.2242\n",
      "46.0% done\n",
      "Loss:     4.03374\n",
      "Val Loss: 4.2232\n",
      "46.2% done\n",
      "Loss:     4.033743333333333\n",
      "Val Loss: 4.2229\n",
      "46.4% done\n",
      "Loss:     4.033736666666666\n",
      "Val Loss: 4.2217\n",
      "46.6% done\n",
      "Loss:     4.033736666666667\n",
      "Val Loss: 4.2200\n",
      "46.8% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2229\n",
      "47.0% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2226\n",
      "47.2% done\n",
      "Loss:     4.033743333333333\n",
      "Val Loss: 4.2250\n",
      "47.4% done\n",
      "Loss:     4.03374\n",
      "Val Loss: 4.2226\n",
      "47.6% done\n",
      "Loss:     4.033743333333333\n",
      "Val Loss: 4.2203\n",
      "47.8% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2220\n",
      "48.0% done\n",
      "Loss:     4.03374\n",
      "Val Loss: 4.2241\n",
      "48.2% done\n",
      "Loss:     4.033733333333334\n",
      "Val Loss: 4.2225\n",
      "48.4% done\n",
      "Loss:     4.03374\n",
      "Val Loss: 4.2250\n",
      "48.6% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2221\n",
      "48.8% done\n",
      "Loss:     4.033743333333333\n",
      "Val Loss: 4.2237\n",
      "49.0% done\n",
      "Loss:     4.033743333333334\n",
      "Val Loss: 4.2240\n",
      "49.2% done\n",
      "Loss:     4.033736666666666\n",
      "Val Loss: 4.2220\n",
      "49.4% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2222\n",
      "49.6% done\n",
      "Loss:     4.03374\n",
      "Val Loss: 4.2247\n",
      "49.8% done\n",
      "Loss:     4.033736666666666\n",
      "Val Loss: 4.2243\n",
      "50.0% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2223\n",
      "50.2% done\n",
      "Loss:     4.03375\n",
      "Val Loss: 4.2240\n",
      "50.4% done\n",
      "Loss:     4.033736666666667\n",
      "Val Loss: 4.2223\n",
      "50.6% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2197\n",
      "50.8% done\n",
      "Loss:     4.033746666666667\n",
      "Val Loss: 4.2207\n",
      "51.0% done\n",
      "Loss:     4.0337499999999995\n",
      "Val Loss: 4.2249\n",
      "51.2% done\n",
      "Loss:     4.033736666666666\n",
      "Val Loss: 4.2235\n",
      "51.4% done\n",
      "Loss:     4.0336799999999995\n",
      "Val Loss: 4.2214\n",
      "51.6% done\n",
      "Loss:     4.033676666666667\n",
      "Val Loss: 4.2245\n",
      "51.8% done\n",
      "Loss:     4.0336766666666675\n",
      "Val Loss: 4.2277\n",
      "52.0% done\n",
      "Loss:     4.033673333333334\n",
      "Val Loss: 4.2198\n",
      "52.2% done\n",
      "Loss:     4.033683333333333\n",
      "Val Loss: 4.2234\n",
      "52.4% done\n",
      "Loss:     4.033623333333333\n",
      "Val Loss: 4.2243\n",
      "52.6% done\n",
      "Loss:     4.033616666666667\n",
      "Val Loss: 4.2198\n",
      "52.8% done\n",
      "Loss:     4.033613333333333\n",
      "Val Loss: 4.2231\n",
      "53.0% done\n",
      "Loss:     4.033606666666667\n",
      "Val Loss: 4.2243\n",
      "53.2% done\n",
      "Loss:     4.033606666666667\n",
      "Val Loss: 4.2195\n",
      "53.4% done\n",
      "Loss:     4.033616666666666\n",
      "Val Loss: 4.2227\n",
      "53.6% done\n",
      "Loss:     4.033603333333334\n",
      "Val Loss: 4.2238\n",
      "53.8% done\n",
      "Loss:     4.033363333333333\n",
      "Val Loss: 4.2233\n",
      "54.0% done\n",
      "Loss:     4.033356666666667\n",
      "Val Loss: 4.2232\n",
      "54.2% done\n",
      "Loss:     4.033346666666667\n",
      "Val Loss: 4.2233\n",
      "54.4% done\n",
      "Loss:     4.03335\n",
      "Val Loss: 4.2205\n",
      "54.6% done\n",
      "Loss:     4.0333533333333325\n",
      "Val Loss: 4.2220\n",
      "54.8% done\n",
      "Loss:     4.033353333333333\n",
      "Val Loss: 4.2227\n",
      "55.0% done\n",
      "Loss:     4.033356666666667\n",
      "Val Loss: 4.2228\n",
      "55.2% done\n",
      "Loss:     4.033356666666667\n",
      "Val Loss: 4.2244\n",
      "55.4% done\n",
      "Loss:     4.03335\n",
      "Val Loss: 4.2224\n",
      "55.6% done\n",
      "Loss:     4.0333533333333325\n",
      "Val Loss: 4.2203\n",
      "55.8% done\n",
      "Loss:     4.03335\n",
      "Val Loss: 4.2225\n",
      "56.0% done\n",
      "Loss:     4.033346666666667\n",
      "Val Loss: 4.2220\n",
      "56.2% done\n",
      "Loss:     4.033346666666667\n",
      "Val Loss: 4.2236\n",
      "56.4% done\n",
      "Loss:     4.033353333333333\n",
      "Val Loss: 4.2274\n",
      "56.6% done\n",
      "Loss:     4.033353333333333\n",
      "Val Loss: 4.2208\n",
      "56.8% done\n",
      "Loss:     4.033296666666666\n",
      "Val Loss: 4.2249\n",
      "57.0% done\n",
      "Loss:     4.033223333333333\n",
      "Val Loss: 4.2222\n",
      "57.2% done\n",
      "Loss:     4.033226666666667\n",
      "Val Loss: 4.2226\n",
      "57.4% done\n",
      "Loss:     4.033219999999999\n",
      "Val Loss: 4.2219\n",
      "57.6% done\n",
      "Loss:     4.033166666666666\n",
      "Val Loss: 4.2256\n",
      "57.8% done\n",
      "Loss:     4.0331633333333325\n",
      "Val Loss: 4.2233\n",
      "58.0% done\n",
      "Loss:     4.033156666666666\n",
      "Val Loss: 4.2222\n",
      "58.2% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2240\n",
      "58.4% done\n",
      "Loss:     4.033156666666666\n",
      "Val Loss: 4.2198\n",
      "58.6% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2235\n",
      "58.8% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2226\n",
      "59.0% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2220\n",
      "59.2% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2202\n",
      "59.4% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2211\n",
      "59.6% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2205\n",
      "59.8% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2209\n",
      "60.0% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2233\n",
      "60.2% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2249\n",
      "60.4% done\n",
      "Loss:     4.0331600000000005\n",
      "Val Loss: 4.2216\n",
      "60.6% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2268\n",
      "60.8% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2232\n",
      "61.0% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2220\n",
      "61.2% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2223\n",
      "61.4% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2189\n",
      "61.6% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2205\n",
      "61.8% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2202\n",
      "62.0% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2216\n",
      "62.2% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2221\n",
      "62.4% done\n",
      "Loss:     4.0331633333333325\n",
      "Val Loss: 4.2225\n",
      "62.6% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2216\n",
      "62.8% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2219\n",
      "63.0% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2207\n",
      "63.2% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2190\n",
      "63.4% done\n",
      "Loss:     4.0331600000000005\n",
      "Val Loss: 4.2231\n",
      "63.6% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2242\n",
      "63.8% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2233\n",
      "64.0% done\n",
      "Loss:     4.033163333333333\n",
      "Val Loss: 4.2226\n",
      "64.2% done\n",
      "Loss:     4.033156666666666\n",
      "Val Loss: 4.2230\n",
      "64.4% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2199\n",
      "64.6% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2211\n",
      "64.8% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2240\n",
      "65.0% done\n",
      "Loss:     4.033146666666666\n",
      "Val Loss: 4.2229\n",
      "65.2% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2207\n",
      "65.4% done\n",
      "Loss:     4.033156666666667\n",
      "Val Loss: 4.2243\n",
      "65.6% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2214\n",
      "65.8% done\n",
      "Loss:     4.03315\n",
      "Val Loss: 4.2246\n",
      "66.0% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2193\n",
      "66.2% done\n",
      "Loss:     4.033153333333333\n",
      "Val Loss: 4.2224\n",
      "66.4% done\n",
      "Loss:     4.033156666666666\n",
      "Val Loss: 4.2214\n",
      "66.6% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2206\n",
      "66.8% done\n",
      "Loss:     4.033153333333334\n",
      "Val Loss: 4.2253\n",
      "67.0% done\n",
      "Loss:     4.033096666666667\n",
      "Val Loss: 4.2220\n",
      "67.2% done\n",
      "Loss:     4.033083333333333\n",
      "Val Loss: 4.2239\n",
      "67.4% done\n",
      "Loss:     4.03309\n",
      "Val Loss: 4.2197\n",
      "67.6% done\n",
      "Loss:     4.033086666666667\n",
      "Val Loss: 4.2222\n",
      "67.8% done\n",
      "Loss:     4.033086666666667\n",
      "Val Loss: 4.2257\n",
      "68.0% done\n",
      "Loss:     4.03309\n",
      "Val Loss: 4.2218\n",
      "68.2% done\n",
      "Loss:     4.033086666666667\n",
      "Val Loss: 4.2209\n",
      "68.4% done\n",
      "Loss:     4.033080000000001\n",
      "Val Loss: 4.2232\n",
      "68.6% done\n",
      "Loss:     4.03309\n",
      "Val Loss: 4.2221\n",
      "68.8% done\n",
      "Loss:     4.033036666666666\n",
      "Val Loss: 4.2243\n",
      "69.0% done\n",
      "Loss:     4.033026666666666\n",
      "Val Loss: 4.2207\n",
      "69.2% done\n",
      "Loss:     4.033026666666666\n",
      "Val Loss: 4.2232\n",
      "69.4% done\n",
      "Loss:     4.033026666666666\n",
      "Val Loss: 4.2204\n",
      "69.6% done\n",
      "Loss:     4.0330200000000005\n",
      "Val Loss: 4.2220\n",
      "69.8% done\n",
      "Loss:     4.032966666666666\n",
      "Val Loss: 4.2251\n",
      "70.0% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2240\n",
      "70.2% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2209\n",
      "70.4% done\n",
      "Loss:     4.03296\n",
      "Val Loss: 4.2212\n",
      "70.6% done\n",
      "Loss:     4.032963333333333\n",
      "Val Loss: 4.2247\n",
      "70.8% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2220\n",
      "71.0% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2233\n",
      "71.2% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2219\n",
      "71.4% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2223\n",
      "71.6% done\n",
      "Loss:     4.032963333333333\n",
      "Val Loss: 4.2231\n",
      "71.8% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2238\n",
      "72.0% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2204\n",
      "72.2% done\n",
      "Loss:     4.032963333333333\n",
      "Val Loss: 4.2242\n",
      "72.4% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2243\n",
      "72.6% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2233\n",
      "72.8% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2233\n",
      "73.0% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2227\n",
      "73.2% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2236\n",
      "73.4% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2220\n",
      "73.6% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2239\n",
      "73.8% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2210\n",
      "74.0% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2209\n",
      "74.2% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2225\n",
      "74.4% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2260\n",
      "74.6% done\n",
      "Loss:     4.032956666666666\n",
      "Val Loss: 4.2228\n",
      "74.8% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2251\n",
      "75.0% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2221\n",
      "75.2% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2231\n",
      "75.4% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2239\n",
      "75.6% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2244\n",
      "75.8% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2215\n",
      "76.0% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2219\n",
      "76.2% done\n",
      "Loss:     4.03296\n",
      "Val Loss: 4.2223\n",
      "76.4% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2198\n",
      "76.6% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2230\n",
      "76.8% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2229\n",
      "77.0% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2233\n",
      "77.2% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2201\n",
      "77.4% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2230\n",
      "77.6% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2218\n",
      "77.8% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2237\n",
      "78.0% done\n",
      "Loss:     4.032959999999999\n",
      "Val Loss: 4.2248\n",
      "78.2% done\n",
      "Loss:     4.032956666666667\n",
      "Val Loss: 4.2229\n",
      "78.4% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2190\n",
      "78.6% done\n",
      "Loss:     4.03296\n",
      "Val Loss: 4.2239\n",
      "78.8% done\n",
      "Loss:     4.032953333333333\n",
      "Val Loss: 4.2204\n",
      "79.0% done\n",
      "Loss:     4.03286\n",
      "Val Loss: 4.2290\n",
      "79.2% done\n",
      "Loss:     4.0328333333333335\n",
      "Val Loss: 4.2264\n",
      "79.4% done\n",
      "Loss:     4.03277\n",
      "Val Loss: 4.2234\n",
      "79.6% done\n",
      "Loss:     4.032763333333333\n",
      "Val Loss: 4.2218\n",
      "79.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2214\n",
      "80.0% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2227\n",
      "80.2% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2210\n",
      "80.4% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2237\n",
      "80.6% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2191\n",
      "80.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2238\n",
      "81.0% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2258\n",
      "81.2% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2196\n",
      "81.4% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2235\n",
      "81.6% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2209\n",
      "81.8% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2234\n",
      "82.0% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2194\n",
      "82.2% done\n",
      "Loss:     4.032763333333333\n",
      "Val Loss: 4.2208\n",
      "82.4% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2207\n",
      "82.6% done\n",
      "Loss:     4.032766666666666\n",
      "Val Loss: 4.2213\n",
      "82.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2210\n",
      "83.0% done\n",
      "Loss:     4.032766666666666\n",
      "Val Loss: 4.2218\n",
      "83.2% done\n",
      "Loss:     4.032763333333333\n",
      "Val Loss: 4.2190\n",
      "83.4% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2248\n",
      "83.6% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2228\n",
      "83.8% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2225\n",
      "84.0% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2190\n",
      "84.2% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2176\n",
      "84.4% done\n",
      "Loss:     4.032753333333334\n",
      "Val Loss: 4.2225\n",
      "84.6% done\n",
      "Loss:     4.03276\n",
      "Val Loss: 4.2198\n",
      "84.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2228\n",
      "85.0% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2199\n",
      "85.2% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2230\n",
      "85.4% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2214\n",
      "85.6% done\n",
      "Loss:     4.032766666666666\n",
      "Val Loss: 4.2216\n",
      "85.8% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2213\n",
      "86.0% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2231\n",
      "86.2% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2238\n",
      "86.4% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2231\n",
      "86.6% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2215\n",
      "86.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2227\n",
      "87.0% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2214\n",
      "87.2% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2213\n",
      "87.4% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2233\n",
      "87.6% done\n",
      "Loss:     4.03276\n",
      "Val Loss: 4.2224\n",
      "87.8% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2241\n",
      "88.0% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2219\n",
      "88.2% done\n",
      "Loss:     4.0327633333333335\n",
      "Val Loss: 4.2216\n",
      "88.4% done\n",
      "Loss:     4.03276\n",
      "Val Loss: 4.2236\n",
      "88.6% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2219\n",
      "88.8% done\n",
      "Loss:     4.032766666666667\n",
      "Val Loss: 4.2198\n",
      "89.0% done\n",
      "Loss:     4.032753333333334\n",
      "Val Loss: 4.2223\n",
      "89.2% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2203\n",
      "89.4% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2193\n",
      "89.6% done\n",
      "Loss:     4.032766666666666\n",
      "Val Loss: 4.2210\n",
      "89.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2205\n",
      "90.0% done\n",
      "Loss:     4.03276\n",
      "Val Loss: 4.2205\n",
      "90.2% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2207\n",
      "90.4% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2220\n",
      "90.6% done\n",
      "Loss:     4.032756666666667\n",
      "Val Loss: 4.2230\n",
      "90.8% done\n",
      "Loss:     4.032760000000001\n",
      "Val Loss: 4.2219\n",
      "91.0% done\n",
      "Loss:     4.032710000000001\n",
      "Val Loss: 4.2212\n",
      "91.2% done\n",
      "Loss:     4.032703333333333\n",
      "Val Loss: 4.2190\n",
      "91.4% done\n",
      "Loss:     4.032703333333334\n",
      "Val Loss: 4.2221\n",
      "91.6% done\n",
      "Loss:     4.0327\n",
      "Val Loss: 4.2189\n",
      "91.8% done\n",
      "Loss:     4.0327\n",
      "Val Loss: 4.2213\n",
      "92.0% done\n",
      "Loss:     4.0326966666666655\n",
      "Val Loss: 4.2227\n",
      "92.2% done\n",
      "Loss:     4.032696666666667\n",
      "Val Loss: 4.2212\n",
      "92.4% done\n",
      "Loss:     4.032696666666667\n",
      "Val Loss: 4.2190\n",
      "92.6% done\n",
      "Loss:     4.0327\n",
      "Val Loss: 4.2202\n",
      "92.8% done\n",
      "Loss:     4.032696666666667\n",
      "Val Loss: 4.2216\n",
      "93.0% done\n",
      "Loss:     4.032696666666667\n",
      "Val Loss: 4.2207\n",
      "93.2% done\n",
      "Loss:     4.032643333333334\n",
      "Val Loss: 4.2229\n",
      "93.4% done\n",
      "Loss:     4.032636666666666\n",
      "Val Loss: 4.2227\n",
      "93.6% done\n",
      "Loss:     4.03263\n",
      "Val Loss: 4.2227\n",
      "93.8% done\n",
      "Loss:     4.032636666666667\n",
      "Val Loss: 4.2206\n",
      "94.0% done\n",
      "Loss:     4.03263\n",
      "Val Loss: 4.2243\n",
      "94.2% done\n",
      "Loss:     4.03264\n",
      "Val Loss: 4.2213\n",
      "94.4% done\n",
      "Loss:     4.032636666666667\n",
      "Val Loss: 4.2229\n",
      "94.6% done\n",
      "Loss:     4.03263\n",
      "Val Loss: 4.2229\n",
      "94.8% done\n",
      "Loss:     4.032633333333333\n",
      "Val Loss: 4.2212\n",
      "95.0% done\n",
      "Loss:     4.032633333333333\n",
      "Val Loss: 4.2233\n",
      "95.2% done\n",
      "Loss:     4.03251\n",
      "Val Loss: 4.2221\n",
      "95.4% done\n",
      "Loss:     4.032506666666667\n",
      "Val Loss: 4.2216\n",
      "95.6% done\n",
      "Loss:     4.0324599999999995\n",
      "Val Loss: 4.2222\n",
      "95.8% done\n",
      "Loss:     4.032446666666667\n",
      "Val Loss: 4.2201\n",
      "96.0% done\n",
      "Loss:     4.03244\n",
      "Val Loss: 4.2195\n",
      "96.2% done\n",
      "Loss:     4.032443333333334\n",
      "Val Loss: 4.2192\n",
      "96.4% done\n",
      "Loss:     4.0324333333333335\n",
      "Val Loss: 4.2215\n",
      "96.6% done\n",
      "Loss:     4.0324366666666664\n",
      "Val Loss: 4.2206\n",
      "96.8% done\n",
      "Loss:     4.0324366666666664\n",
      "Val Loss: 4.2198\n",
      "97.0% done\n",
      "Loss:     4.0324366666666664\n",
      "Val Loss: 4.2221\n",
      "97.2% done\n",
      "Loss:     4.032369999999999\n",
      "Val Loss: 4.2221\n",
      "97.4% done\n",
      "Loss:     4.0323633333333335\n",
      "Val Loss: 4.2197\n",
      "97.6% done\n",
      "Loss:     4.032373333333333\n",
      "Val Loss: 4.2209\n",
      "97.8% done\n",
      "Loss:     4.032373333333333\n",
      "Val Loss: 4.2222\n",
      "98.0% done\n",
      "Loss:     4.032373333333333\n",
      "Val Loss: 4.2189\n",
      "98.2% done\n",
      "Loss:     4.032369999999999\n",
      "Val Loss: 4.2192\n",
      "98.4% done\n",
      "Loss:     4.032366666666666\n",
      "Val Loss: 4.2213\n",
      "98.6% done\n",
      "Loss:     4.032366666666666\n",
      "Val Loss: 4.2202\n",
      "98.8% done\n",
      "Loss:     4.082966666666667\n",
      "Val Loss: 4.2598\n",
      "99.0% done\n",
      "Loss:     4.034013333333333\n",
      "Val Loss: 4.2169\n",
      "99.2% done\n",
      "Loss:     4.033416666666666\n",
      "Val Loss: 4.2121\n",
      "99.4% done\n",
      "Loss:     4.033226666666667\n",
      "Val Loss: 4.2141\n",
      "99.6% done\n",
      "Loss:     4.033076666666666\n",
      "Val Loss: 4.2120\n",
      "99.8% done\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "for epoch in range(25000):  # loop over the dataset multiple times\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss_list = []\n",
    "    # forward + backward + optimize\n",
    "    for i in range(math.ceil(train_set.shape[0] / BATCH_SIZE)):\n",
    "        \n",
    "        outputs = model(train_set[i*BATCH_SIZE:(1+i)*BATCH_SIZE])\n",
    "        loss = criterion(outputs, train_answers[i*BATCH_SIZE:(1+i)*BATCH_SIZE])\n",
    "\n",
    "        loss_list.append(float(str(loss).split(\"(\")[1].split(\",\")[0]))\n",
    "\n",
    "        loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    avg_loss_list.append(np.average(loss_list))\n",
    "    val_outputs = model(val_set)\n",
    "    val_loss = criterion(val_outputs, val_answers)\n",
    "    \n",
    "    val_loss_list.append(float(str(val_loss).split(\"(\")[1].split(\",\")[0]))\n",
    "    \n",
    "    # print statistics\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Loss:     \" + str(avg_loss_list[-1]))\n",
    "        print(\"Val Loss: \" + str(val_loss).split(\"(\")[1].split(\",\")[0])\n",
    "        print(str(epoch/250) + \"% done\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x225a29064c0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_loss_list)\n",
    "plt.plot(val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct (Accuracy): 0.8209677419354838\n"
     ]
    }
   ],
   "source": [
    "outputs = model(val_set)\n",
    "\n",
    "maxes = np.argmax(outputs.to('cpu').detach().numpy(), axis = 1)\n",
    "correct_guesses = [maxes[i] == val_answer[i] for i in range(len(maxes))]\n",
    "\n",
    "print(\"Percent correct (Accuracy): \" + str(correct_guesses.count(True)/len(correct_guesses)))\n",
    "\n",
    "torch.save(model, '.\\\\models\\\\LSTM_wide')\n",
    "torch.save({'epoch': 25000, 'model': model, 'optimizer': optimizer}, '.\\\\models\\\\LSTM_wide_checkpoint')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
