{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import urllib\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import re \n",
    "\n",
    "# Download word vectorizer\n",
    "vectors = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "\n",
    "\n",
    "# Download data\n",
    "def get_response(url):\n",
    "    operUrl = urllib.request.urlopen(url)\n",
    "    if(operUrl.getcode()==200):\n",
    "        data = operUrl.read()\n",
    "    else:\n",
    "        print(\"Error receiving data\", operUrl.getcode())\n",
    "    return data\n",
    "\n",
    "data = get_response('https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json')\n",
    "\n",
    "\n",
    "# Load and combine data into train, val, and test sets\n",
    "json_data = json.loads(data)\n",
    "val = json_data['val']\n",
    "oos_val = json_data['oos_val']\n",
    "test = json_data['test']\n",
    "oos_test = json_data['oos_test']\n",
    "train = json_data['train']\n",
    "oos_train = json_data['oos_train']\n",
    "\n",
    "\n",
    "for i in oos_train:\n",
    "    train.append(i) \n",
    "\n",
    "for i in oos_val:\n",
    "    val.append(i) \n",
    "\n",
    "for i in oos_test:\n",
    "    test.append(i) \n",
    "\n",
    "### Strip data of symbols and convert to lists of strings    \n",
    "# Create regex pattern\n",
    "pattern = re.compile('[^\\w\\s]')\n",
    "\n",
    "# Define function to apply regex pattern over each dataset\n",
    "def string_separation(data):\n",
    "    return [pattern.sub('', data[i][0]).split(\" \") \\\n",
    "     for i, j in enumerate(data)]\n",
    "\n",
    "# Apply function to each dataset\n",
    "train_separated_values = string_separation(train)\n",
    "val_separated_values = string_separation(val)\n",
    "test_separated_values = string_separation(test)\n",
    "\n",
    "\n",
    "### vectorize each value using word2vec\n",
    "\n",
    "# Create function that checks if a string can be vectorized against the vectorizer\n",
    "# If it can, vectorize it, otherwise return a 0.0 array of equal size\n",
    "def get_vector(string):\n",
    "    try: \n",
    "        value = vectors.get_vector(string)\n",
    "        return value\n",
    "    except:\n",
    "        return np.array([0.0] * 50)\n",
    "    \n",
    "# Create a function that applies the get_vector function across a list of values\n",
    "def convert_to_vectors(series):\n",
    "    new_series = []\n",
    "\n",
    "    for i in series:\n",
    "        if i == '':\n",
    "            pass\n",
    "        else:\n",
    "            new_series.append(get_vector(i))\n",
    "            \n",
    "    return new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that applies convert_to_vectors across an entire dataset\n",
    "def vectorize_samples(data):\n",
    "    return [convert_to_vectors(data[i]) for i, j in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize each dataset\n",
    "train_X_arrays = vectorize_samples(train_separated_values)\n",
    "val_X_arrays = vectorize_samples(val_separated_values)\n",
    "test_X_arrays = vectorize_samples(test_separated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since values are being assigned to Tensors establish the storage location by checking for GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Establish padding length (Could have been as low as 30 given data set 50 was chosen to increase usability)\n",
    "pad_length = 50\n",
    "\n",
    "# Create an empty array to be used for padding values\n",
    "pad_value = np.array([0.0] * 50, dtype=np.float32)\n",
    "\n",
    "# Define function that takes a list, pad_length, and pad_value to create a new array of values with 0 padded values up to the pad_length\n",
    "def adjust_padding(some_list, pad_length=pad_length, pad_value=pad_value):\n",
    "    some_list = list(some_list)\n",
    "    while len(some_list) < pad_length:\n",
    "        some_list.append(pad_value)\n",
    "    return np.array(some_list)\n",
    "\n",
    "# Apply the function over each dataset\n",
    "train_padded      = [adjust_padding(i) for i in train_X_arrays]\n",
    "val_padded        = [adjust_padding(val_X_arrays[i]) for i, j in enumerate(val_X_arrays)]\n",
    "test_padded       = [adjust_padding(test_X_arrays[i]) for i, j in enumerate(test_X_arrays)]\n",
    "\n",
    "# Convert arrays to tensors to feed model\n",
    "\n",
    "train_set         = torch.tensor(train_padded).reshape((15100,1,50,50)).float().to(device)\n",
    "val_set           = torch.tensor(val_padded).reshape((3100,1,50,50)).float().to(device)\n",
    "test_set          = torch.tensor(test_padded).reshape((5500,1,50,50)).float().to(device)\n",
    "\n",
    "# Create tensors of answers\n",
    "\n",
    "train_answer      = [j[1] for i, j in enumerate(train)]\n",
    "val_answer        = [j[1] for i, j in enumerate(val)]\n",
    "test_answer       = [j[1] for i, j in enumerate(test)]\n",
    "\n",
    "### Create an index to train against\n",
    "\n",
    "# Create dictionary map of each unique answer type\n",
    "chosen_dict = {j: i for i, j in enumerate(list(set([train[i][1] for i, j in enumerate(train)])))}\n",
    "\n",
    "# Use dictionary to consistently label answer data\n",
    "for item in chosen_dict.keys():\n",
    "    for i, j in enumerate(train_answer):\n",
    "        if item == j:\n",
    "            train_answer[i] = chosen_dict[item]\n",
    "    for i, j in enumerate(val_answer):\n",
    "        if item == j:\n",
    "            val_answer[i] = chosen_dict[item]\n",
    "    for i, j in enumerate(test_answer):\n",
    "        if item == j:\n",
    "            test_answer[i] = chosen_dict[item]\n",
    "\n",
    "# Convert labeled data into tensors\n",
    "train_answers = torch.tensor(train_answer).to(device)\n",
    "val_answers = torch.tensor(val_answer).to(device)\n",
    "test_answers = torch.tensor(test_answer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build model in PyTorch\n",
    "\n",
    "class ClassifierModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        \n",
    "        self.wide_conv = nn.Conv2d(1, 32, (5, 50))\n",
    "        self.narrow_conv1 = nn.Conv2d(1, 32, (3, 50))\n",
    "        self.narrow_conv2 = nn.Conv1d(32, 32, (3))\n",
    "        self.conv2 = nn.Conv1d(64, 128, (3))\n",
    "        self.linear1 = nn.Linear(128*44, 256)\n",
    "        self.linear2 = nn.Linear(256, 128)\n",
    "        self.probs_pre = nn.Linear(128, 151) # 151 output classes\n",
    "        self.probabilities = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use wide (5) word convolution to capture longer-term dependencies\n",
    "        y = F.elu(self.wide_conv(x))\n",
    "        y = y.view(y.shape[0:3])\n",
    "        # Use short (3) word convolution to capture neighboring relationships\n",
    "        z = F.elu(self.narrow_conv1(x))\n",
    "        z = z.view(z.shape[0:3])\n",
    "        z = F.elu(self.narrow_conv2(z))\n",
    "        # Concatenate results\n",
    "        x = torch.cat((y, z), 1)\n",
    "        # Perform one additional convolution to evaluate the connection between dependencies\n",
    "        x = F.elu(self.conv2(x))\n",
    "        # Pass to Feed Forward ANN to produce probabilities of each class\n",
    "        # Use dropout to reduce overfitting and assist model with training on un-seen data\n",
    "        x = x.view(-1, 128*44)\n",
    "        x = F.elu(self.dropout(self.linear1(x)))\n",
    "        x = F.elu(self.dropout(self.linear2(x)))\n",
    "        x = self.probabilities(self.probs_pre(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:     5.0173\n",
      "Val Loss: 5.0172\n",
      "0.0% done\n",
      "Loss:     4.5673\n",
      "Val Loss: 4.6670\n",
      "1.0% done\n",
      "Loss:     4.4738\n",
      "Val Loss: 4.6038\n",
      "2.0% done\n",
      "Loss:     4.4552\n",
      "Val Loss: 4.5827\n",
      "3.0% done\n",
      "Loss:     4.4413\n",
      "Val Loss: 4.5674\n",
      "4.0% done\n",
      "Loss:     4.4224\n",
      "Val Loss: 4.5588\n",
      "5.0% done\n",
      "Loss:     4.3956\n",
      "Val Loss: 4.5482\n",
      "6.0% done\n",
      "Loss:     4.3794\n",
      "Val Loss: 4.5325\n",
      "7.0% done\n",
      "Loss:     4.3730\n",
      "Val Loss: 4.5303\n",
      "8.0% done\n",
      "Loss:     4.3591\n",
      "Val Loss: 4.5154\n",
      "9.0% done\n",
      "Loss:     4.3326\n",
      "Val Loss: 4.5006\n",
      "10.0% done\n",
      "Loss:     4.3057\n",
      "Val Loss: 4.4729\n",
      "11.0% done\n",
      "Loss:     4.2726\n",
      "Val Loss: 4.4515\n",
      "12.0% done\n",
      "Loss:     4.2718\n",
      "Val Loss: 4.4496\n",
      "13.0% done\n",
      "Loss:     4.2653\n",
      "Val Loss: 4.4416\n",
      "14.0% done\n",
      "Loss:     4.2585\n",
      "Val Loss: 4.4434\n",
      "15.0% done\n",
      "Loss:     4.2333\n",
      "Val Loss: 4.4328\n",
      "16.0% done\n",
      "Loss:     4.2127\n",
      "Val Loss: 4.4127\n",
      "17.0% done\n",
      "Loss:     4.2056\n",
      "Val Loss: 4.4100\n",
      "18.0% done\n",
      "Loss:     4.1986\n",
      "Val Loss: 4.4002\n",
      "19.0% done\n",
      "Loss:     4.1858\n",
      "Val Loss: 4.3982\n",
      "20.0% done\n",
      "Loss:     4.1857\n",
      "Val Loss: 4.3970\n",
      "21.0% done\n",
      "Loss:     4.1736\n",
      "Val Loss: 4.3913\n",
      "22.0% done\n",
      "Loss:     4.1657\n",
      "Val Loss: 4.3800\n",
      "23.0% done\n",
      "Loss:     4.1656\n",
      "Val Loss: 4.3771\n",
      "24.0% done\n",
      "Loss:     4.1592\n",
      "Val Loss: 4.3762\n",
      "25.0% done\n",
      "Loss:     4.1522\n",
      "Val Loss: 4.3754\n",
      "26.0% done\n",
      "Loss:     4.1467\n",
      "Val Loss: 4.3672\n",
      "27.0% done\n",
      "Loss:     4.1326\n",
      "Val Loss: 4.3566\n",
      "28.0% done\n",
      "Loss:     4.1126\n",
      "Val Loss: 4.3445\n",
      "29.0% done\n",
      "Loss:     4.1125\n",
      "Val Loss: 4.3396\n",
      "30.0% done\n",
      "Loss:     4.1058\n",
      "Val Loss: 4.3385\n",
      "31.0% done\n",
      "Loss:     4.0992\n",
      "Val Loss: 4.3316\n",
      "32.0% done\n",
      "Loss:     4.0924\n",
      "Val Loss: 4.3292\n",
      "33.0% done\n",
      "Loss:     4.0924\n",
      "Val Loss: 4.3307\n",
      "34.0% done\n",
      "Loss:     4.0925\n",
      "Val Loss: 4.3337\n",
      "35.0% done\n",
      "Loss:     4.0859\n",
      "Val Loss: 4.3210\n",
      "36.0% done\n",
      "Loss:     4.0794\n",
      "Val Loss: 4.3088\n",
      "37.0% done\n",
      "Loss:     4.0728\n",
      "Val Loss: 4.3144\n",
      "38.0% done\n",
      "Loss:     4.0727\n",
      "Val Loss: 4.3106\n",
      "39.0% done\n",
      "Loss:     4.0726\n",
      "Val Loss: 4.3135\n",
      "40.0% done\n",
      "Loss:     4.0724\n",
      "Val Loss: 4.3095\n",
      "41.0% done\n",
      "Loss:     4.0724\n",
      "Val Loss: 4.3159\n",
      "42.0% done\n",
      "Loss:     4.0657\n",
      "Val Loss: 4.3109\n",
      "43.0% done\n",
      "Loss:     4.0591\n",
      "Val Loss: 4.3031\n",
      "44.0% done\n",
      "Loss:     4.0524\n",
      "Val Loss: 4.2978\n",
      "45.0% done\n",
      "Loss:     4.0524\n",
      "Val Loss: 4.2978\n",
      "46.0% done\n",
      "Loss:     4.0523\n",
      "Val Loss: 4.2950\n",
      "47.0% done\n",
      "Loss:     4.0457\n",
      "Val Loss: 4.2958\n",
      "48.0% done\n",
      "Loss:     4.0456\n",
      "Val Loss: 4.2953\n",
      "49.0% done\n",
      "Loss:     4.0456\n",
      "Val Loss: 4.2870\n",
      "50.0% done\n",
      "Loss:     4.0391\n",
      "Val Loss: 4.2899\n",
      "51.0% done\n",
      "Loss:     4.0390\n",
      "Val Loss: 4.2831\n",
      "52.0% done\n",
      "Loss:     4.0324\n",
      "Val Loss: 4.2809\n",
      "53.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2856\n",
      "54.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2819\n",
      "55.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2843\n",
      "56.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2749\n",
      "57.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2770\n",
      "58.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2729\n",
      "59.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2733\n",
      "60.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2758\n",
      "61.0% done\n",
      "Loss:     4.0321\n",
      "Val Loss: 4.2755\n",
      "62.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2727\n",
      "63.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2747\n",
      "64.0% done\n",
      "Loss:     4.0322\n",
      "Val Loss: 4.2752\n",
      "65.0% done\n",
      "Loss:     4.0321\n",
      "Val Loss: 4.2782\n",
      "66.0% done\n",
      "Loss:     4.0321\n",
      "Val Loss: 4.2724\n",
      "67.0% done\n",
      "Loss:     4.0321\n",
      "Val Loss: 4.2808\n",
      "68.0% done\n",
      "Loss:     4.0321\n",
      "Val Loss: 4.2697\n",
      "69.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2838\n",
      "70.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2740\n",
      "71.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2671\n",
      "72.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2747\n",
      "73.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2800\n",
      "74.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2749\n",
      "75.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2751\n",
      "76.0% done\n",
      "Loss:     4.0321\n",
      "Val Loss: 4.2722\n",
      "77.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2696\n",
      "78.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2729\n",
      "79.0% done\n",
      "Loss:     4.0320\n",
      "Val Loss: 4.2767\n",
      "80.0% done\n",
      "Loss:     4.0319\n",
      "Val Loss: 4.2714\n",
      "81.0% done\n",
      "Loss:     4.0319\n",
      "Val Loss: 4.2681\n",
      "82.0% done\n",
      "Loss:     4.0319\n",
      "Val Loss: 4.2718\n",
      "83.0% done\n",
      "Loss:     4.0319\n",
      "Val Loss: 4.2717\n",
      "84.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2697\n",
      "85.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2712\n",
      "86.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2736\n",
      "87.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2729\n",
      "88.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2746\n",
      "89.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2711\n",
      "90.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2677\n",
      "91.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2650\n",
      "92.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2692\n",
      "93.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2709\n",
      "94.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2777\n",
      "95.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2746\n",
      "96.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2720\n",
      "97.0% done\n",
      "Loss:     4.0318\n",
      "Val Loss: 4.2718\n",
      "98.0% done\n",
      "Loss:     4.0317\n",
      "Val Loss: 4.2690\n",
      "99.0% done\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, optimizer, and loss criteria and begin first iteration of training.\n",
    "\n",
    "model = ClassifierModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, amsgrad=True)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(50000):  # loop over the dataset multiple times\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(train_set)\n",
    "    loss = criterion(outputs, train_answers)\n",
    "    \n",
    "    loss_list.append(float(str(loss).split(\"(\")[1].split(\",\")[0]))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val_outputs = model(val_set)\n",
    "    val_loss = criterion(val_outputs, val_answers)\n",
    "    \n",
    "    val_loss_list.append(float(str(val_loss).split(\"(\")[1].split(\",\")[0]))\n",
    "    \n",
    "    # print statistics\n",
    "    if epoch % 500 == 0:\n",
    "        print(\"Loss:     \" + str(loss).split(\"(\")[1].split(\",\")[0])\n",
    "        print(\"Val Loss: \" + str(val_loss).split(\"(\")[1].split(\",\")[0])\n",
    "        print(str(epoch/500) + \"% done\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Save initial training results\n",
    "\n",
    "torch.save(model, '.\\\\models\\\\proper_vectors.1_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2c9dd014790>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if additional training is likely to yield improvements\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.plot(val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct (Accuracy): 0.7651612903225806\n"
     ]
    }
   ],
   "source": [
    "# Test model on validation set for estimated performance\n",
    "outputs = model(val_set)\n",
    "\n",
    "maxes = np.argmax(outputs.to('cpu').detach().numpy(), axis = 1)\n",
    "correct_guesses = [maxes[i] == val_answer[i] for i in range(len(maxes))]\n",
    "\n",
    "print(\"Percent correct (Accuracy): \" + str(correct_guesses.count(True)/len(correct_guesses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
