{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent prediction:\n",
    "\n",
    "`This portion as well as the other markdown cell were written outside of the 2-hour time limit. Some extraneous code was deleted from the final result. All code including comments were present before the execution of the training cell`\n",
    "\n",
    "### Tell us about your favorite machine learning project:\n",
    "\n",
    "One of my favorite projects was a speech recognition neural net I worked. I had been programming and learning Data Science for a little over two months at the time. [GitHub to Project](https://github.com/Zethtren/Speech_Recognition_Transcription_Translation)\n",
    "\n",
    "#### What were the goals and focuses of this project?\n",
    "\n",
    "The goal of this project was to vuild, from scratch, a series of models for predicting which language was being spoken, what was being said in an audio file, and translating it to another language.\n",
    "\n",
    "#### Describe the technical details related to the project, such as: the input features; model architectures; algorithms; metrics; optimizers; performance evaluation; etcâ€¦ \n",
    "\n",
    "I collected data from a couple of sources. (See Github for additional information on this or any other questions you may have). For the first two models I used Short-Time Fourier transformed audio segments (as 2D numpy arrays) as inputs to the models. The architecture for the language prediction was a feed forward CNN It took the 2d numpy array as input and produce a softmax prob of which language was spoken ( I achieved 98% accuracy here). \n",
    "\n",
    "The architecture for the transcribor was an LSTM which took a very similar numpy array as input and returned a sequence of single character spelling the content of the audio. I only achieved 40% accuracy here but, the model would have benefited greatly from longer training and increased data samples. \n",
    "\n",
    "I tried a combination of SGD and Adam optimizers and models were evaluated strictly on accuracy.\n",
    "\n",
    "Baseline for RNN was (1/28)% ~ 3.6%\n",
    "\n",
    "#### What were some novel approaches that you employed while solving the problem?\n",
    "\n",
    "I didn't really take too many novel approaches to the problem. It was difficult to find resources on building speech transcription models so a lot the model architecture was guess and check. I've had a number of ideas on how to improve it since but have not had the time or finances to pursue them as a primary goal.\n",
    "\n",
    "#### What kinds of results did you produce?\n",
    "\n",
    "98% accuracy when detecting which language was spoken between English and German\n",
    "\n",
    "40% accuracy on transcribing text. \n",
    "\n",
    "Did not attempt tranlastion as I was unsatisfied with the transcription result. (Many more resources exist on translating than transcribing)\n",
    "\n",
    "#### What would you change about this project?\n",
    "\n",
    "A lot. I was satisfied with the first model and would definitely wish to re-train it on a wider breadth of languages.\n",
    "\n",
    "For the transcription model I've had a lot of ideas to explore.\n",
    "First I would like to see if I could set a threshold value (for silence) to separate words across a speech file. This would allow me to build models that predict words instead of letters. I have a feeling this would be much easier.\n",
    "\n",
    "If the audio has to come in a stream I would definitely like to spend the time to make the samples smaller and train it to predict just a hnadful of characters at a time. This would decrease dependencies on previously spoken letters and allow the model to train faster and better. It would also allow it to be served in a manner which feels more natural or reactive instead of waiting until the sentence is finished. \n",
    "\n",
    "I've discussed more about it in my project page as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import urllib\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a pre-built word-vector map\n",
    "vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.2871e-01, -1.0834e-01,  2.1497e-01, -5.0237e-01,  1.0379e-01,\n",
       "        2.2728e-01, -5.4198e-01, -2.9008e-01, -6.4607e-01,  1.2664e-01,\n",
       "       -4.1487e-01, -2.9343e-01,  3.6855e-01, -4.1733e-01,  6.9116e-01,\n",
       "        6.7341e-02,  1.9715e-01, -3.0465e-02, -2.1723e-01, -1.2238e+00,\n",
       "        9.5469e-03,  1.9594e-01,  5.6595e-01, -6.7473e-02,  5.9208e-02,\n",
       "       -1.3909e+00, -8.9275e-01, -1.3546e-01,  1.6200e-01, -4.0210e-01,\n",
       "        4.1644e+00,  3.7816e-01,  1.5797e-01, -4.8892e-01,  2.3131e-01,\n",
       "        2.3258e-01, -2.5314e-01, -1.9977e-01, -1.2258e-01,  1.5620e-01,\n",
       "       -3.1995e-01,  3.8314e-01,  4.7266e-01,  8.7700e-01,  3.2223e-01,\n",
       "        1.3292e-03, -4.9860e-01,  5.5580e-01, -7.0359e-01, -5.2693e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show use of vector\n",
    "# Converts a value into a 50 dimensional vector embedding\n",
    "# These can be built smaller around specific vocabulary\n",
    "vectors.get_vector('some')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and read in the data\n",
    "\n",
    "def get_response(url):\n",
    "    operUrl = urllib.request.urlopen(url)\n",
    "    if(operUrl.getcode()==200):\n",
    "        data = operUrl.read()\n",
    "    else:\n",
    "        print(\"Error receiving data\", operUrl.getcode())\n",
    "    return data\n",
    "\n",
    "data = get_response('https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oos_val', 'val', 'train', 'oos_test', 'test', 'oos_train'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into corresponding segments\n",
    "\n",
    "val = json_data['val']\n",
    "test = json_data['test']\n",
    "train = json_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set containing 20 random choices from the complete set\n",
    "# Use random seed for reproducability\n",
    "\n",
    "pre_choice_set = set([train[i][1] for i, j in enumerate(train)])\n",
    "pre_choice_set = list(pre_choice_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_set = set()\n",
    "random.seed(42)\n",
    "while len(chosen_set) < 20:\n",
    "    chosen_set.add(random.choice(pre_choice_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bill_balance',\n",
       " 'calendar_update',\n",
       " 'change_user_name',\n",
       " 'exchange_rate',\n",
       " 'freeze_account',\n",
       " 'goodbye',\n",
       " 'ingredients_list',\n",
       " 'lost_luggage',\n",
       " 'meeting_schedule',\n",
       " 'pto_used',\n",
       " 'reminder_update',\n",
       " 'report_lost_card',\n",
       " 'routing',\n",
       " 'schedule_maintenance',\n",
       " 'share_location',\n",
       " 'spelling',\n",
       " 'timezone',\n",
       " 'todo_list_update',\n",
       " 'what_can_i_ask_you',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word sets from passed strings\n",
    "\n",
    "train = [train[i] for i, j in enumerate(train) if train[i][1] in chosen_set]\n",
    "test  = [test[i] for i, j in enumerate(test) if test[i][1] in chosen_set]\n",
    "val   = [val[i] for i, j in enumerate(val) if val[i][1] in chosen_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I cannot shuffle from here on out otherwise the values will not line up\n",
    "train_split_words = [train[i][0].split(\" \") for i, j in enumerate(train)]\n",
    "val_split_words   = [val[i][0].split(\" \") for i, j in enumerate(val)]\n",
    "test_split_words  = [test[i][0].split(\" \") for i, j in enumerate(test)]\n",
    "train_answer      = [train[i][1] for i, j in enumerate(train)]\n",
    "val_answer        = [val[i][1] for i, j in enumerate(val)]\n",
    "test_answer       = [test[i][1] for i, j in enumerate(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a case to handle missing words (Mostly numbers in this sample set)\n",
    "\n",
    "def get_vector(string):\n",
    "    try: \n",
    "        value = vectors.get_vector(string)\n",
    "        return value\n",
    "    except:\n",
    "        return np.array([0.0] * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary value roughly double the actual max\n",
    "\n",
    "pad_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_value = np.array([0.0] * 50)\n",
    "\n",
    "train_vectors = [[get_vector(train_split_words[k][i]) for i, j in enumerate(train_split_words[k])] for k, _ in enumerate(train_split_words)]\n",
    "val_vectors =   [[get_vector(val_split_words[k][i]) for i, j in enumerate(val_split_words[k])] for k, _ in enumerate(val_split_words)]\n",
    "test_vectors =  [[get_vector(test_split_words[k][i]) for i, j in enumerate(test_split_words[k])] for k, _ in enumerate(test_split_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_padding(some_list, pad_length=pad_length, pad_value=pad_value):\n",
    "    some_list = some_list\n",
    "    while len(some_list) < pad_length:\n",
    "        some_list.append(pad_value)\n",
    "    return some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create padded training val and test sets so they will fit into fixed size model\n",
    "\n",
    "train_padded = [adjust_padding(train_vectors[i]) for i, j in enumerate(train_vectors)]\n",
    "val_padded   = [adjust_padding(val_vectors[i]) for i, j in enumerate(val_vectors)]\n",
    "test_padded  = [adjust_padding(test_vectors[i]) for i, j in enumerate(test_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a ConvNet for classifying the texts\n",
    "# Could also use an RNN but due to two hour time-limit and data shaping issues\n",
    "# with feeding a many-to-one model I opted for ConvNet\n",
    "\n",
    "class ClassifierModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ClassifierModel, self).__init__()\n",
    "\n",
    "        self.state_input = nn.Conv2d(1, 4, (3, 50))\n",
    "        self.conv2 = nn.Conv1d(4, 8, (3))\n",
    "        self.conv3 = nn.Conv1d(8, 16, (3))\n",
    "        self.linear1 = nn.Linear(704, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.probs_pre = nn.Linear(32, 20)\n",
    "        self.probabilities = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.state_input(x))\n",
    "        x = x.view(x.shape[0:3])\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(-1, 16*44)\n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.probabilities(self.probs_pre(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierModel().float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dict = {j: i for i, j in enumerate(chosen_set)}\n",
    "\n",
    "for item in chosen_dict.keys():\n",
    "    for i, j in enumerate(train_answer):\n",
    "        if item == j:\n",
    "            train_answer[i] = chosen_dict[item]\n",
    "    for i, j in enumerate(val_answer):\n",
    "        if item == j:\n",
    "            val_answer[i] = chosen_dict[item]\n",
    "    for i, j in enumerate(test_answer):\n",
    "        if item == j:\n",
    "            test_answer[i] = chosen_dict[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(2.9958, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.9957, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.7369, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.7384, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6558, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.7014, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6323, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6665, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6042, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6444, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6012, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6467, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6002, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6479, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5989, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6483, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5981, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6469, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5976, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6418, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5963, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6498, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6130, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6379, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5855, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6330, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5843, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6302, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5832, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6324, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5702, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6163, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5411, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5942, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5375, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5914, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5364, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5953, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5359, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5961, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5355, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5966, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5353, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5969, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5350, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5948, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5348, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5948, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5346, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5944, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5346, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5970, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5337, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5979, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5325, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5946, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5315, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5963, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5313, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5964, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5310, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5979, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5306, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5956, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5309, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5958, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5303, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5963, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5301, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5989, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5299, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5988, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5303, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5977, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5297, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5973, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5297, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5979, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5296, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5989, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5294, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6008, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5294, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6004, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5289, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6022, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5285, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6019, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5282, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6002, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5282, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5978, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.7963, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.8135, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.6075, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6388, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5473, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5858, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5435, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5867, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5425, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5872, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5420, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5878, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5418, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5884, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5434, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5853, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5415, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5912, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5411, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5929, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5409, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5932, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5407, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5932, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5404, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5929, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5043, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5753, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4991, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5764, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4962, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5739, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4950, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5725, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4947, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5705, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4944, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5705, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4938, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5707, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4936, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5700, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4929, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5698, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4923, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5690, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4922, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5685, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4921, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5682, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4920, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5681, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4920, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5677, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4883, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5663, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4875, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5629, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4874, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5643, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.5257, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.6068, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4863, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5622, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4382, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5201, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4358, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5233, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4348, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5263, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4336, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5252, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4331, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5245, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4324, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5238, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4319, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5235, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4312, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5226, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4310, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5260, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4301, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5255, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4288, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5304, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4244, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5262, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.4360, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5220, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3941, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.5006, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3928, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4893, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3921, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4890, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3919, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4872, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3917, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4875, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3916, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4872, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3914, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4886, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3913, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4887, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(2.3911, grad_fn=<NllLossBackward>)\n",
      "Val Loss: tensor(2.4917, grad_fn=<NllLossBackward>)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0075)\n",
    "\n",
    "\n",
    "for epoch in range(5000):  # loop over the dataset multiple times\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(torch.tensor(train_padded).reshape((2000,1,50,50)).float().to(device))\n",
    "    loss = criterion(outputs.to('cpu'), torch.tensor(train_answer))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val_outputs = model(torch.tensor(val_padded).reshape((400,1,50,50)).float().to(device))\n",
    "    val_loss = criterion(val_outputs.to('cpu'), torch.tensor(val_answer))\n",
    "    \n",
    "    # print statistics\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Loss: \" + str(loss))\n",
    "        print(\"Val Loss: \" + str(val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of time limit\n",
    "\n",
    "I started the training on this model just before the 2 hour deadline. There were a couple of iterations on learning rates.\n",
    "\n",
    "One learning rate using the AdamW faired a little better initially but also seemed to cap off. I'd definitely like to modify the architecture and optimizer a little bit because I feel the results could be better. Once its done training as I established here I will run an accuracy test on the test set. Although, I'm not expecting great results.\n",
    "\n",
    "Given more time. I would have attempted an LSTM based architecture that evaluated the final hidden layer. This would have reduced the need for quite as much prep-work.\n",
    "\n",
    "I'm also going to look across the data-set and use this [TF-IDF](https://github.com/Zethtren/NLP_Exploration) class I built about a week ago. I opted against this since th3e challenge indicted the desire to make this function as a deployable model. \n",
    "\n",
    "TF-IDF requires the entire dataset to compare against for document frequency. This implementation above only requires that there be fewer than 50 words. This could also be modified. \n",
    "\n",
    "(This would have given me the fastest and easiest results)\n",
    "\n",
    "As stated an LSTM model would allow for more flexibility in ingestion, having no requirements on sentence length). However the cost is a more complicated architecture with longer training and prediction times. Although I know from past usage it would likely be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor(test_padded).reshape((600,1,50,50)).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = np.argmax(outputs.to('cpu').detach().numpy(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_guesses = [maxes[i] == test_answer[i] for i in range(len(maxes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_guesses.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_guesses.count(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct (Accuracy): 0.5633333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent correct (Accuracy): \" + str(correct_guesses.count(True)/len(correct_guesses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above I knew the result wouldn't be great but 56.3% Isn't terrible for a pre-liminary model. And there is a lot that was clearly working. \n",
    "\n",
    "A custom reduced dimension embedding layer could improve accuracy. \n",
    "Increased amount of data and increase in training time would likely have also seen benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
